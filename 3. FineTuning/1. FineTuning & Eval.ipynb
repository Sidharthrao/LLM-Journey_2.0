{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f7b56d",
   "metadata": {},
   "source": [
    "### 1. Hugging Face Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765497c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: bad assignment\n"
     ]
    }
   ],
   "source": [
    "# Set your Hugging Face token as an environment variable\n",
    "# For Linux/Mac: export HF_TOKEN=\"your_hf_token_here\"\n",
    "# For Windows: set HF_TOKEN=your_hf_token_here\n",
    "# Or use: os.environ['HF_TOKEN'] = \"your_hf_token_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e825e",
   "metadata": {},
   "source": [
    "For window - \n",
    "import os\n",
    "os.environ['HF_TOKEN'] = \"token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9a8d8",
   "metadata": {},
   "source": [
    "### 2. Check for latest version of datasets library in pypl, if it is different from the one in requriements.txt upgrade the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dd7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "344bb8e6",
   "metadata": {},
   "source": [
    "### 3.A loading datasets from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57088325",
   "metadata": {},
   "source": [
    "Link - https://huggingface.co/datasets\n",
    "\n",
    "\n",
    "Dataset used - knkarthick/dialogsum (Check in search bar)\n",
    "\n",
    "\n",
    "**Reason for selecting this dataset →**  \n",
    "a. Data contains long-form conversations paired with human-written summaries, which act as reference sentences. Using these references,  \n",
    "b. we evaluate Large Language Models (LLMs) on their ability to generate accurate summaries.  \n",
    "c. The LLM-generated summaries (candidate sentences) are compared against the human references.  \n",
    "d. BLEU and ROUGE scores are then calculated to measure similarity and assess the quality of the model’s summarization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015dec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth’s MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e25094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd31edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd562c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training dataset\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02869048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev_0',\n",
       " 'dialogue': \"#Person1#: Hello, how are you doing today?\\n#Person2#: I ' Ve been having trouble breathing lately.\\n#Person1#: Have you had any type of cold lately?\\n#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\\n#Person1#: Do you have any allergies that you know of?\\n#Person2#: No, I don ' t have any allergies that I know of.\\n#Person1#: Does this happen all the time or mostly when you are active?\\n#Person2#: It happens a lot when I work out.\\n#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\\n#Person2#: Thank you for your help, doctor.\",\n",
       " 'summary': '#Person2# has trouble breathing. The doctor asks #Person2# about it and will send #Person2# to a pulmonary specialist.',\n",
       " 'topic': 'see a doctor'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset\n",
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c023f",
   "metadata": {},
   "source": [
    "### 3.B loading model from hugging face to help summarize the conversation \n",
    "\n",
    "We use model from Hugging face -> Flan-T5 (Base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de34359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth’s MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/sidharthrao/Documents/Documents - Sidharth’s MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#This will load the model from hugging face and convert it to bfloat16 precision, which is faster for training. without any changes to the model architecture or parameters.\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # this is the tokenizer related to flanT5 model (we know tokenizer is different for each model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96537848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644fd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa67b782",
   "metadata": {},
   "source": [
    "### 4. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19c4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# This def function to check the percentage of trainable model parameters is finetuned\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af780bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"] # Reference sentence\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\") # Converting the prompt into tokens\n",
    "output = tokenizer.decode(original_model.generate(inputs[\"input_ids\"], max_new_tokens=200)[0], skip_special_tokens=True) \n",
    "\n",
    "print(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a9afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f079f66",
   "metadata": {},
   "source": [
    "#### a. Calcualte BLEU and ROUGE score directly on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ff8ee",
   "metadata": {},
   "source": [
    "#### b. Perform Full Finetuning and Calcualte BLEU and ROUGE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941c07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d97534ac",
   "metadata": {},
   "source": [
    "#### c. Perform PEFT and Calcualte BLEU and ROUGE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7532030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter Efficient Fine-Tuning (PEFT)\n",
    "#Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05, # Dropping out 5% of the neurons in DL network\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23089c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "#Add LoRA adapter layers/parameters to the original LLM to be trained.\n",
    "peft_model = get_peft_model(original_model,\n",
    "                            lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model)) # This is fine tuning only 1.41% of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd457e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3538.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "\n",
    "\n",
    "\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df9cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 6527.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True) # this code helps to filter the dataset to only 100th dialogue in the dataset. This is done to reduce the dataset size to 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a801a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b1618",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb4d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "    num_rows: 1246\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# Define the fraction of the dataset you want to use (e.g., 10%)\n",
    "subset_fraction = 0.1\n",
    "\n",
    "# Calculate the number of samples to use\n",
    "subset_size = int(len(dataset[\"train\"]) * subset_fraction)\n",
    "\n",
    "# Create a subset of the dataset\n",
    "subset_dataset = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))\n",
    "\n",
    "# Tokenize the subset\n",
    "tokenized_subset = subset_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "print(tokenized_subset)\n",
    "# Define training arguments and create Trainer instance.\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=4,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    max_steps=10\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_subset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f406c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth’s MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/Users/sidharthrao/Documents/Documents - Sidharth’s MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 10%|█         | 1/10 [09:39<1:26:55, 579.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 49.75, 'learning_rate': 0.0009000000000000001, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [19:38<1:18:49, 591.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 48.0, 'learning_rate': 0.0008, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [29:16<1:08:15, 585.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 42.0, 'learning_rate': 0.0007, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [38:53<58:10, 581.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 40.0, 'learning_rate': 0.0006, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [48:27<48:15, 579.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 34.75, 'learning_rate': 0.0005, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [58:02<38:30, 577.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 33.25, 'learning_rate': 0.0004, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [1:17:59<38:59, 779.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 31.0, 'learning_rate': 0.0003, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:37:26<30:06, 903.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 29.375, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [2:05:10<19:01, 1141.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 28.875, 'learning_rate': 0.0001, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [2:14:53<00:00, 809.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 29.125, 'learning_rate': 0.0, 'epoch': 0.06}\n",
      "{'train_runtime': 8093.451, 'train_samples_per_second': 0.01, 'train_steps_per_second': 0.001, 'train_loss': 36.6125, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e10177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Base model\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "#Finetuned model\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       './peft-dialogue-summary-checkpoint-local',\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_traiable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b8e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'd like to upgrade my computer. #Person2#: I'm not sure what exactly I'd need. #Person1#: I'd like to add a painting program to my software. #Person2#: I'd like to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "# We will use the same 200th Index to find the results of the finetuned model. and calculate the BLEU and ROUGE scores.\n",
    "\n",
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"] # Reference sentence\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input prompt and make sure it's on the same device as the model\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(original_model.device)\n",
    "\n",
    "# Generate text from the original model\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=200,\n",
    "        num_beams=2,\n",
    "        do_sample=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decode the output text\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print results\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec252f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2432c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'd like to upgrade my computer. #Person2#: I'm not sure what exactly I'd need. #Person1#: I'd like to add a painting program to my software. #Person2#: I'd like to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "Upgrade your computer.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We will use the same 200th Index to find the results of the finetuned model. and calculate the BLEU and ROUGE scores.\n",
    "\n",
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"] # Reference sentence\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the input prompt and make sure it's on the same device as the models\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(original_model.device)\n",
    "\n",
    "# Move the PEFT model to the same device as the original model\n",
    "peft_model = peft_model.to(original_model.device)\n",
    "\n",
    "# Generate summaries using the original model and PEFT model\n",
    "with torch.no_grad():\n",
    "    original_model_outputs = original_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=2,\n",
    "            do_sample=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Decode the model outputs\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print all results\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{peft_model_text_output}')\n",
    "print(dash_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0cf1c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value('string'), 'references': List(Value('string'))}, {'predictions': Value('string'), 'references': Value('string')}], usage: \"\"\"\n",
      "Calculates average rouge scores for a list of hypotheses and references\n",
      "Args:\n",
      "    predictions: list of predictions to score. Each prediction\n",
      "        should be a string with tokens separated by spaces.\n",
      "    references: list of reference for each prediction. Each\n",
      "        reference should be a string with tokens separated by spaces.\n",
      "    rouge_types: A list of rouge types to calculate.\n",
      "        Valid names:\n",
      "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
      "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
      "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
      "\"`.\n",
      "        See details in https://github.com/huggingface/datasets/issues/617\n",
      "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
      "    use_aggregator: Return aggregates if this is set to True\n",
      "Returns:\n",
      "    rouge1: rouge_1 (f1),\n",
      "    rouge2: rouge_2 (f1),\n",
      "    rougeL: rouge_l (f1),\n",
      "    rougeLsum: rouge_lsum (f1)\n",
      "Examples:\n",
      "\n",
      "    >>> rouge = evaluate.load('rouge')\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "# Calculating BLEU and ROUGE scores\n",
    "\n",
    "rouge_score = evaluate.load('rouge')\n",
    "\n",
    "print(rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf3a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_FTandEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
