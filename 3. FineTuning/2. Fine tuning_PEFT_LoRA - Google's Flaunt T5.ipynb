{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52526caf",
   "metadata": {},
   "source": [
    "## 1. Installing relevant packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474bac79",
   "metadata": {},
   "source": [
    "https://github.dev/amitkumaryada/BIA/blob/main/Fine_tune_FLAN_T5_with_PEFT_LoRA_(deeplearning_ai).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92628434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~penai (/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch==2.7.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: torchdata==0.8.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.8.0)\n",
      "Requirement already satisfied: transformers==4.36.2 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (4.36.2)\n",
      "Requirement already satisfied: datasets==4.0.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: rouge_score==0.1.2 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: loralib==0.1.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.1.1)\n",
      "Requirement already satisfied: peft==0.7.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: fsspec in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.34.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.7.1)\n",
      "Requirement already satisfied: pandas in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (2.3.1)\n",
      "Requirement already satisfied: tqdm in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (3.10.5)\n",
      "Requirement already satisfied: sentencepiece in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: protobuf in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (6.31.1)\n",
      "Requirement already satisfied: accelerate==0.26.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.26.1)\n",
      "Requirement already satisfied: filelock in ./.venv_FTandEval/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 1)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv_FTandEval/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv_FTandEval/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv_FTandEval/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: urllib3>=1.25 in ./.venv_FTandEval/lib/python3.11/site-packages (from torchdata==0.8.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: requests in ./.venv_FTandEval/lib/python3.11/site-packages (from torchdata==0.8.0->-r requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from transformers==4.36.2->-r requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 4)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: xxhash in ./.venv_FTandEval/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv_FTandEval/lib/python3.11/site-packages (from datasets==4.0.0->-r requirements.txt (line 4)) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in ./.venv_FTandEval/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: absl-py in ./.venv_FTandEval/lib/python3.11/site-packages (from rouge_score==0.1.2->-r requirements.txt (line 6)) (2.3.1)\n",
      "Requirement already satisfied: nltk in ./.venv_FTandEval/lib/python3.11/site-packages (from rouge_score==0.1.2->-r requirements.txt (line 6)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from rouge_score==0.1.2->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: psutil in ./.venv_FTandEval/lib/python3.11/site-packages (from peft==0.7.1->-r requirements.txt (line 8)) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv_FTandEval/lib/python3.11/site-packages (from huggingface_hub->-r requirements.txt (line 10)) (1.1.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv_FTandEval/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (3.12.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv_FTandEval/lib/python3.11/site-packages (from requests->torchdata==0.8.0->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv_FTandEval/lib/python3.11/site-packages (from requests->torchdata==0.8.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv_FTandEval/lib/python3.11/site-packages (from requests->torchdata==0.8.0->-r requirements.txt (line 2)) (2025.7.14)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv_FTandEval/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv_FTandEval/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 14)) (3.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv_FTandEval/lib/python3.11/site-packages (from jinja2->torch==2.7.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: click in ./.venv_FTandEval/lib/python3.11/site-packages (from nltk->rouge_score==0.1.2->-r requirements.txt (line 6)) (8.2.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~penai (/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~penai (/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888a2b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "W0810 07:02:04.553000 6840 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5df408",
   "metadata": {},
   "source": [
    "#### a. Loading datasets from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6d71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "# dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43204c96",
   "metadata": {},
   "source": [
    "#### b. Loading Google's Flaunt T5 model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498c7914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c20119",
   "metadata": {},
   "source": [
    "#### c. Providing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21aade9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To make pasta, you can use a steamed pasta.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert the input prompt into token IDs that the model can understand\n",
    "inputs = tokenizer(\"A step by step recipe to make pasta:\", return_tensors=\"pt\")\n",
    "\n",
    "# Step 2: Use the model to generate a continuation of the prompt\n",
    "# This outputs a tensor containing token IDs of the generated text\n",
    "outputs = original_model.generate(**inputs)\n",
    "\n",
    "# Step 3: Convert the output token IDs back into readable text, removing special tokens\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc4943",
   "metadata": {},
   "source": [
    "#### d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2136d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Returns the number and percentage of trainable parameters in a PyTorch model.\n",
    "    \"\"\"\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n\" \\\n",
    "           f\"all model parameters: {all_model_params}\\n\" \\\n",
    "           f\"percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "# Call the function on your model\n",
    "print(print_number_of_trainable_model_parameters(original_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b62191",
   "metadata": {},
   "source": [
    "#### e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad38f76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "How would you like to upgrade your computer?\n"
     ]
    }
   ],
   "source": [
    "# Select the 200th test example from the dataset\n",
    "index = 200\n",
    "\n",
    "# Extract the dialogue (input text) and summary (ground truth) from the test set\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Create a prompt in instruction format to guide the model for summarization\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and return it as PyTorch tensors (required for model input)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate a summary using the model\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],     # Provide tokenized input IDs to the model\n",
    "        max_new_tokens=200       # Set a cap on how many tokens the model should generate\n",
    "    )[0],                        # Get the first (and only) generated sequence\n",
    "    skip_special_tokens=True     # Remove tokens like <pad>, <eos> from output\n",
    ")\n",
    "\n",
    "# Create a visual separator for better readability of printed output\n",
    "dash_line = '-' * 100  # Corrected from original code\n",
    "\n",
    "# Print the original input prompt\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "\n",
    "# Print the human-written (reference) summary\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "\n",
    "# Print the summary generated by the model (zero-shot)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2eaa0",
   "metadata": {},
   "source": [
    "#### f. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a12e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom tokenization function to prepare the input and target sequences for training\n",
    "def tokenize_function(example):\n",
    "    # Define the beginning and end of the prompt to frame the input\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    # Combine the prompt with each dialogue in the batch\n",
    "    # Note: example[\"dialogue\"] is a list when batched=True\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "\n",
    "    # Tokenize the input prompts with padding and truncation\n",
    "    # Output is a dict with keys like input_ids, attention_mask etc.\n",
    "    # We're extracting only input_ids here and assigning them to the example\n",
    "    example['input_ids'] = tokenizer(\n",
    "        prompt,                            # list of full prompts\n",
    "        padding=\"max_length\",              # pad all to max length in tokenizer config\n",
    "        truncation=True,                   # truncate if it exceeds max model input\n",
    "        return_tensors=\"pt\"                # return PyTorch tensors\n",
    "    ).input_ids\n",
    "\n",
    "    # Tokenize the reference summaries (targets) similarly\n",
    "    example['labels'] = tokenizer(\n",
    "        example[\"summary\"],                # list of ground truth summaries\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    # Return the modified example with input_ids and labels\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09afdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08724673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7783a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "output_dir=\"./results\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Set up training arguments for the Trainer API\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,       # Where to save checkpoints and logs\n",
    "    learning_rate=1e-5,          # Learning rate for optimizer\n",
    "    num_train_epochs=2,          # Total number of training epochs\n",
    "    weight_decay=0.01,           # Regularization to avoid overfitting\n",
    "    logging_steps=1,             # Log every 1 step (frequent logging for debugging)\n",
    "    max_steps=1,                 # üîÅ WARNING: Only runs 1 training step! Use for quick test only!\n",
    "    report_to=None,              # Disables logging to WandB, TensorBoard etc.\n",
    "    # save = \"./checkpoint\"      # ‚ùå Not a valid Trainer arg; remove or replace with `save_strategy`\n",
    ")\n",
    "\n",
    "# Initialize the Trainer class with model, training arguments, and datasets\n",
    "trainer = Trainer(\n",
    "    model=original_model,                     # The model to fine-tune\n",
    "    args=training_args,                       # Training configurations\n",
    "    train_dataset=tokenized_datasets['train'],   # Training data\n",
    "    eval_dataset=tokenized_datasets['validation'] # Validation data for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84709a",
   "metadata": {},
   "source": [
    "#### g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46a35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea4f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Define Global Parameters\n",
    "# --------------------------\n",
    "L_RATE = 3e-4                  # Learning rate for the AdamW optimizer\n",
    "BATCH_SIZE = 8                 # Training batch size per device (GPU/CPU)\n",
    "PER_DEVICE_EVAL_BATCH = 4      # Evaluation batch size per device\n",
    "WEIGHT_DECAY = 0.01            # L2 regularization strength to prevent overfitting\n",
    "SAVE_TOTAL_LIM = 3             # Keep only the latest 3 saved checkpoints\n",
    "NUM_EPOCHS = 3                 # Total number of training epochs\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# --------------------------\n",
    "# Set up Training Arguments\n",
    "# --------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",             # Directory to save checkpoints and logs\n",
    "   save_strategy=\"epoch\",              # Save model at the end of every epoch\n",
    "   evaluation_strategy=\"epoch\",        # Run evaluation at the end of every epoch\n",
    "   learning_rate=L_RATE,               # Learning rate for training\n",
    "   per_device_train_batch_size=BATCH_SIZE,  # Training batch size per GPU/CPU\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,  # Evaluation batch size\n",
    "   weight_decay=WEIGHT_DECAY,          # Weight decay for regularization\n",
    "   save_total_limit=SAVE_TOTAL_LIM,    # Keep only last N checkpoints (avoid clutter)\n",
    "   num_train_epochs=NUM_EPOCHS,        # Number of epochs to train the model\n",
    "   predict_with_generate=True,         # Generate predictions during evaluation (needed for seq2seq)\n",
    "   push_to_hub=False                   # Don‚Äôt push model to Hugging Face Hub after training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb35b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# Initialize the trainer for sequence-to-sequence fine-tuning\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=original_model,                # The pre-trained model to fine-tune (e.g., FLAN-T5)\n",
    "   args=training_args,                  # TrainingArguments or Seq2SeqTrainingArguments configured earlier\n",
    "   train_dataset=tokenized_datasets[\"train\"],        # Tokenized training dataset\n",
    "   eval_dataset=tokenized_datasets[\"validation\"],    # Tokenized validation dataset\n",
    "   tokenizer=tokenizer,                # Required so `generate()` can decode input/output properly\n",
    "\n",
    "   # OPTIONAL: You can use this if you're manually defining a data collator\n",
    "   # This is especially useful if using mixed-length input sequences and want dynamic padding\n",
    "   # data_collator=data_collator,\n",
    "\n",
    "   # OPTIONAL: Use this to compute custom metrics like ROUGE during evaluation\n",
    "   # Needs to be a function that takes EvalPrediction and returns a dict of metric values\n",
    "   # compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1891afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3996a42dc3f4a1f829e6968b6206471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaaa7d2bc2e44f1872a9e0eeaff32dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-16 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.053508281707764, 'eval_runtime': 0.2883, 'eval_samples_per_second': 17.342, 'eval_steps_per_second': 6.937, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe15ae1308d469ab3188fd1367b5291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-32 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.704914093017578, 'eval_runtime': 0.1838, 'eval_samples_per_second': 27.2, 'eval_steps_per_second': 10.88, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/.venv_FTandEval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbb8bff31f048c5b4653d99b1dc1749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-48 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.192478656768799, 'eval_runtime': 0.1836, 'eval_samples_per_second': 27.231, 'eval_steps_per_second': 10.893, 'epoch': 3.0}\n",
      "{'train_runtime': 55.3771, 'train_samples_per_second': 6.772, 'train_steps_per_second': 0.867, 'train_loss': 9.056327819824219, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=9.056327819824219, metrics={'train_runtime': 55.3771, 'train_samples_per_second': 6.772, 'train_steps_per_second': 0.867, 'train_loss': 9.056327819824219, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec4e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. FineTuning & Eval.ipynb\n",
      "2. Fine tuning_PEFT_LoRA - Google's Flaunt T5.ipynb\n",
      "\u001b[34mDataSet\u001b[m\u001b[m\n",
      "\u001b[34mpeft-dialogue-summary-checkpoint-local\u001b[m\u001b[m\n",
      "\u001b[34mpeft-dialogue-summary-training-1753710836\u001b[m\u001b[m\n",
      "\u001b[34mpeft-dialogue-summary-training-1753795343\u001b[m\u001b[m\n",
      "requirements.txt\n",
      "\u001b[34mresults\u001b[m\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f95fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = original_model.to('cpu') #manually moving the model to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee2ad6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"/Users/sidharthrao/Documents/Documents - Sidharth‚Äôs MacBook Pro/GitHub/Project-Dash/2. GenAI and AgenticAI/ii. Class Sessions/d. Finetuning & Evaluation/results/checkpoint-16\", torch_dtype=torch.bfloat16).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37207691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL (e.g., base FLAN-T5):\n",
      "Sharing programs are a great way to make up your own flyers and banners. #Person1#Person2#Person2#Person3#Person3#Person3#Person3#Person3#Person3#Person3#Person2#Person3#Person3#Person3#Person3#Person2#Person3#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#P\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL (e.g., fine-tuned FLAN-T5):\n",
      "Person is a person.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 1: Select test example from dataset\n",
    "# --------------------------------------------------\n",
    "index = 200  # Index of the sample you want to evaluate\n",
    "\n",
    "# Extract the dialogue and its human-written summary\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 2: Prepare the summarization prompt\n",
    "# --------------------------------------------------\n",
    "# This prompt works well with instruction-tuned models like FLAN-T5\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 3: Tokenize the prompt to get input IDs\n",
    "# --------------------------------------------------\n",
    "# Convert the prompt into PyTorch tensors compatible with the model\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 4: Generate output from the original model\n",
    "# --------------------------------------------------\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=200,     # Cap the number of generated tokens\n",
    "        num_beams=1             # Greedy decoding (no beam search)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decode model output (token IDs) to readable text\n",
    "original_model_text_output = tokenizer.decode(\n",
    "    original_model_outputs[0], skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 5: Generate output from the instruction-tuned model\n",
    "# --------------------------------------------------\n",
    "instruct_model_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=200,\n",
    "        num_beams=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decode instruct model output\n",
    "instruct_model_text_output = tokenizer.decode(\n",
    "    instruct_model_outputs[0], skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 6: Print results side-by-side for qualitative comparison\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a horizontal line separator for cleaner printing\n",
    "dash_line = '-' * 100\n",
    "\n",
    "# Print the human reference summary\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "\n",
    "# Print the output from the original model\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL (e.g., base FLAN-T5):\\n{original_model_text_output}')\n",
    "\n",
    "# Print the output from the instruction-tuned model\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL (e.g., fine-tuned FLAN-T5):\\n{instruct_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88322a3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9b909a",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520cde36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value('string'), 'references': List(Value('string'))}, {'predictions': Value('string'), 'references': Value('string')}], usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (f1),\n",
       "    rouge2: rouge_2 (f1),\n",
       "    rougeL: rouge_l (f1),\n",
       "    rougeLsum: rouge_lsum (f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = evaluate.load('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561af325",
   "metadata": {},
   "source": [
    "https://github.dev/amitkumaryada/BIA/blob/main/Fine_tune_FLAN_T5_with_PEFT_LoRA_(deeplearning_ai).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3711d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097fa0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>@Person1#Person2#Person2#Person3#Person3#Perso...</td>\n",
       "      <td>Happy Birthday, Brian. #Person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "1  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "2  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "3  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "4  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "5  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "6  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "7  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "8  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "9  @Person1#Person2#Person2#Person3#Person3#Perso...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  Employees should be allowed to use Instant Mes...  \n",
       "1  Employees should be allowed to use Instant Mes...  \n",
       "2  Employees should be allowed to use Instant Mes...  \n",
       "3                                Person is a person.  \n",
       "4                                Person is a person.  \n",
       "5                                Person is a person.  \n",
       "6       Masha and Hero are getting divorced. #Person  \n",
       "7       Masha and Hero are getting divorced. #Person  \n",
       "8       Masha and Hero are getting divorced. #Person  \n",
       "9                     Happy Birthday, Brian. #Person  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 10 dialogues from the test dataset\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "\n",
    "# Extract the corresponding first 10 human-written summaries (baseline)\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# Initialize empty lists to store summaries generated by each model\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "# Loop through each dialogue to generate summaries\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    \n",
    "    # Create a summarization prompt for the current dialogue\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    # Tokenize the prompt into input IDs for the model\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # ---------- Original Model Summarization ----------\n",
    "    # Generate output tokens from the original model\n",
    "    original_model_outputs = original_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)  # Limit output length\n",
    "    )\n",
    "    # Decode the token IDs into human-readable text\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    # Append the generated summary to the original model's list\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    # ---------- Instruct Model Summarization ----------\n",
    "    # Generate output tokens from the instruct-tuned model\n",
    "    instruct_model_outputs = instruct_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)  # Limit output length\n",
    "    )\n",
    "    # Decode the token IDs into human-readable text\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    # Append the generated summary to the instruct model's list\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "# Combine human, original model, and instruct model summaries into tuples\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "# Convert the combined list into a pandas DataFrame for comparison and analysis\n",
    "df = pd.DataFrame(zipped_summaries, columns=[\n",
    "    'human_baseline_summaries',\n",
    "    'original_model_summaries',\n",
    "    'instruct_model_summaries'\n",
    "])\n",
    "\n",
    "# Display the DataFrame to compare all three summary types\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889338d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae8bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': np.float64(0.04119581122195633), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.033130979994113734), 'rougeLsum': np.float64(0.03301162146846246)}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': np.float64(0.22960386633398955), 'rouge2': np.float64(0.09172222222222223), 'rougeL': np.float64(0.19423905073401154), 'rougeLsum': np.float64(0.19505676605340655)}\n"
     ]
    }
   ],
   "source": [
    "# ---------- ROUGE Evaluation for Original Model ----------\n",
    "# Compute ROUGE scores comparing:\n",
    "# - predictions: summaries generated by the original model\n",
    "# - references: corresponding human-written summaries (baseline)\n",
    "# The use of `use_aggregator=True` returns the average score across all samples,\n",
    "# and `use_stemmer=True` ensures words are reduced to their base/root form for better matching.\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# ---------- ROUGE Evaluation for Instruct Model ----------\n",
    "# Same process as above, but evaluating the instruct-tuned model's summaries.\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# ---------- Display Results ----------\n",
    "# Print the ROUGE scores for both models for comparison.\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643f924",
   "metadata": {},
   "source": [
    "#### Performing PEFT (Above steps we did Full fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5685d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define the LoRA (Low-Rank Adaptation) configuration for fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=32,                  # Rank of the low-rank matrices.\n",
    "                           # Higher values allow capturing more task-specific information,\n",
    "                           # but increase the number of trainable parameters.\n",
    "\n",
    "    lora_alpha=32,         # Scaling factor for LoRA updates.\n",
    "                           # Controls how much the LoRA weights influence the final output.\n",
    "\n",
    "    target_modules=[\"q\", \"v\"],  # List of target submodules inside the model where LoRA will be applied.\n",
    "                                # \"q\" and \"v\" refer to query and value projection layers in the attention mechanism.\n",
    "\n",
    "    lora_dropout=0.05,     # Dropout probability applied to LoRA layers during training to reduce overfitting.\n",
    "\n",
    "    bias=\"none\",           # Specifies how to handle biases in target modules:\n",
    "                           #   - \"none\" means no bias parameters are adapted.\n",
    "                           #   - \"all\" means all bias parameters are adapted.\n",
    "                           #   - \"lora_only\" adapts only biases for LoRA layers.\n",
    "\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Task type ‚Äî here it's Sequence-to-Sequence Language Modeling\n",
    "                                     # which matches FLAN-T5 (encoder-decoder architecture).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd192d37",
   "metadata": {},
   "source": [
    "#### Reducing parameters to fine tune (Earlier - We took entire 78337408 parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf548051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 1376256\n",
      "all model parameters: 78337408\n",
      "percentage of trainable model parameters: 1.76%\n"
     ]
    }
   ],
   "source": [
    "# Apply the LoRA configuration to the original model\n",
    "# This wraps the base model with LoRA layers so only selected parameters are fine-tuned.\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters vs total parameters\n",
    "# This helps confirm that LoRA is freezing most of the model's weights,\n",
    "# and only a small percentage is actually trainable (reducing compute needs).\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a62e8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcfa2f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Create a unique output directory for saving model checkpoints\n",
    "# The directory name includes a Unix timestamp so each run gets its own folder.\n",
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# Define training arguments for the Hugging Face Trainer\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,        # Where to save checkpoints and logs\n",
    "    auto_find_batch_size=True,    # Automatically find the largest batch size that fits in GPU memory\n",
    "    learning_rate=1e-3,           # Higher learning rate because LoRA fine-tunes fewer parameters\n",
    "    num_train_epochs=1,           # Number of training epochs (full passes through dataset)\n",
    "    logging_steps=1,               # Log metrics after every step (useful for debugging)\n",
    "    max_steps=1                    # Limit total steps to 1 (likely for a quick smoke test)\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for PEFT fine-tuning\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,                              # LoRA-wrapped base model\n",
    "    args=peft_training_args,                       # Training configuration\n",
    "    train_dataset=tokenized_datasets[\"train\"],     # Tokenized training dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b79ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26061858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "730beb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter expects base: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "adapter_dir = \"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "# 1) Read adapter metadata to get the original base checkpoint\n",
    "peft_cfg = PeftConfig.from_pretrained(adapter_dir)\n",
    "print(\"Adapter expects base:\", peft_cfg.base_model_name_or_path)\n",
    "\n",
    "# 2) Load the exact same base + tokenizer used for training\n",
    "base_id = peft_cfg.base_model_name_or_path  # e.g., \"google/flan-t5-base\"\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(base_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_id)\n",
    "\n",
    "# 3) Stitch the adapter onto the correct base\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    adapter_dir,\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# 4) Device for Mac Silicon (MPS) or CPU fallback\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd7d968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c217e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LoRA-adapted PEFT model to CPU\n",
    "peft_model = peft_model.to('cpu')\n",
    "\n",
    "# Move instruct-tuned model to CPU\n",
    "instruct_model = instruct_model.to('cpu')\n",
    "\n",
    "# Move original base model to CPU\n",
    "original_model = original_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7965fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Sharing programs are a great way to make up your own flyers and banners. #Person1#Person2#Person2#Person3#Person3#Person3#Person3#Person3#Person3#Person3#Person2#Person3#Person3#Person3#Person3#Person2#Person3#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#Person3#Person3#Person2#P\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Person is a person.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: Upgrade your computer.\n"
     ]
    }
   ],
   "source": [
    "# Select an example index from the test dataset\n",
    "index = 200\n",
    "\n",
    "# Retrieve the dialogue text for the selected index\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "\n",
    "# Retrieve the human-written baseline summary for the selected index\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Build the prompt that will be passed to each model for summarization\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "# Tokenize the prompt into input IDs for the models\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# ---------------- ORIGINAL MODEL ----------------\n",
    "# Generate the summary using the original model with specified generation settings\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)  # max tokens and beam search config\n",
    ")\n",
    "# Decode the generated token IDs back into text\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ---------------- INSTRUCT MODEL ----------------\n",
    "# Generate the summary using the instruct-tuned model\n",
    "instruct_model_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "# Decode the generated token IDs into text\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ---------------- PEFT (LoRA) MODEL ----------------\n",
    "# Generate the summary using the LoRA fine-tuned model\n",
    "peft_model_outputs = peft_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "# Decode the generated token IDs into text\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print a dashed line separator\n",
    "print(dash_line)\n",
    "\n",
    "# Print the human baseline summary\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')  # NOTE: variable name should match baseline_human_summary above\n",
    "\n",
    "# Print separator\n",
    "print(dash_line)\n",
    "\n",
    "# Print original model's generated summary\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "\n",
    "# Print separator\n",
    "print(dash_line)\n",
    "\n",
    "# Print instruct model's generated summary\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "\n",
    "# Print separator\n",
    "print(dash_line)\n",
    "\n",
    "# Print PEFT model's generated summary\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c6dc541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>@Person1#Person1#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Employees should be allowed to use Instant Mes...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person2#Perso...</td>\n",
       "      <td>Person is a person.</td>\n",
       "      <td>Take public transport to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>@Person1#Person2#Person2#Person2#Person3#Perso...</td>\n",
       "      <td>Masha and Hero are getting divorced. #Person</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>@Person1#Person2#Person2#Person3#Person3#Perso...</td>\n",
       "      <td>Happy Birthday, Brian. #Person</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "1  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "2  @Person1#Person1#Person2#Person2#Person3#Perso...   \n",
       "3  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "4  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "5  @Person1#Person2#Person2#Person2#Person2#Perso...   \n",
       "6  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "7  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "8  @Person1#Person2#Person2#Person2#Person3#Perso...   \n",
       "9  @Person1#Person2#Person2#Person3#Person3#Perso...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  Employees should be allowed to use Instant Mes...   \n",
       "1  Employees should be allowed to use Instant Mes...   \n",
       "2  Employees should be allowed to use Instant Mes...   \n",
       "3                                Person is a person.   \n",
       "4                                Person is a person.   \n",
       "5                                Person is a person.   \n",
       "6       Masha and Hero are getting divorced. #Person   \n",
       "7       Masha and Hero are getting divorced. #Person   \n",
       "8       Masha and Hero are getting divorced. #Person   \n",
       "9                     Happy Birthday, Brian. #Person   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  This memo is to be distributed to all employee...  \n",
       "1  This memo is to be distributed to all employee...  \n",
       "2  This memo is to be distributed to all employee...  \n",
       "3                     Take public transport to work.  \n",
       "4                     Take public transport to work.  \n",
       "5                     Take public transport to work.  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                     Brian's birthday is coming up.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 10 dialogues from the test dataset\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "\n",
    "# Extract the corresponding first 10 human-written summaries (baseline)\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# Initialize empty lists to store model-generated summaries\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "# Loop through each dialogue to generate summaries from all models\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "\n",
    "    # Build the summarization prompt for the current dialogue\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "    # Tokenize the prompt into input IDs for the models\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Retrieve the corresponding human-written summary for reference\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "\n",
    "    # ---------------- ORIGINAL MODEL ----------------\n",
    "    # Generate summary using the original (base) model\n",
    "    original_model_outputs = original_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    # Decode tokens to text\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # ---------------- INSTRUCT MODEL ----------------\n",
    "    # Generate summary using the instruct-tuned model\n",
    "    instruct_model_outputs = instruct_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    # Decode tokens to text\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # ---------------- PEFT (LoRA) MODEL ----------------\n",
    "    # Generate summary using the LoRA fine-tuned model\n",
    "    peft_model_outputs = peft_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    # Decode tokens to text\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append results to respective lists\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "# Combine all summaries into a single list of tuples\n",
    "zipped_summaries = list(zip(\n",
    "    human_baseline_summaries,\n",
    "    original_model_summaries,\n",
    "    instruct_model_summaries,\n",
    "    peft_model_summaries\n",
    "))\n",
    "\n",
    "# Convert the zipped summaries into a DataFrame for easy comparison\n",
    "df = pd.DataFrame(\n",
    "    zipped_summaries,\n",
    "    columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries']\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192d5956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': np.float64(0.04119581122195633), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.033130979994113734), 'rougeLsum': np.float64(0.03301162146846246)}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': np.float64(0.22960386633398955), 'rouge2': np.float64(0.09172222222222223), 'rougeL': np.float64(0.19423905073401154), 'rougeLsum': np.float64(0.19505676605340655)}\n",
      "PEFT MODEL:\n",
      "{'rouge1': np.float64(0.29970979020979016), 'rouge2': np.float64(0.14344664031620552), 'rougeL': np.float64(0.24626456876456876), 'rougeLsum': np.float64(0.24932465682465677)}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric from Hugging Face's 'evaluate' library\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# ---------------- ORIGINAL MODEL ----------------\n",
    "# Compute ROUGE scores comparing:\n",
    "# - predictions: summaries generated by the original model\n",
    "# - references: corresponding human-written baseline summaries\n",
    "# The slicing ensures matching lengths between predictions and references.\n",
    "# 'use_aggregator=True' ‚Üí returns average scores across all samples.\n",
    "# 'use_stemmer=True' ‚Üí applies stemming to handle different word forms (e.g., \"run\" vs. \"running\").\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# ---------------- INSTRUCT MODEL ----------------\n",
    "# Compute ROUGE scores for the instruct-tuned model vs. human summaries\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# ---------------- PEFT (LoRA) MODEL ----------------\n",
    "# Compute ROUGE scores for the LoRA fine-tuned model vs. human summaries\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# ---------------- PRINT RESULTS ----------------\n",
    "# Print the aggregated ROUGE scores for each model\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e97054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140c626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba4729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730059b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b45059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_FTandEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
